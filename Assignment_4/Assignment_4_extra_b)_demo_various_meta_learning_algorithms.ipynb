{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.11"
    },
    "colab": {
      "name": "Assignment_4_extra_b) demo_various_meta_learning_algorithms.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zjzsu2000/CMPE297_Sec49AdvanceDL/blob/master/Assignment_4/Assignment_4_extra_b)_demo_various_meta_learning_algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXVDqWzMkB4-"
      },
      "source": [
        "# Building Relation Network Using Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izwqbfw7kB5C"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1c3oAjpkB5J"
      },
      "source": [
        "classA = np.random.rand(1000,18)\n",
        "\n",
        "ClassB = np.random.rand(1000,18)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfQrymQVkB5R"
      },
      "source": [
        "data = np.vstack([classA, ClassB])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twoMQ2OfkB5X"
      },
      "source": [
        "label = np.vstack([np.ones((len(classA),1)),np.zeros((len(ClassB),1))])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC3kD48JkB5c"
      },
      "source": [
        "So, our dataset will have 2000 records"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fT15LJCkB5d",
        "outputId": "f0346f17-a5a3-4080-d95a-c34e9cfe43fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 18)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l9vd9MMkB5g",
        "outputId": "38974782-6767-4bf6-bf48-082f554bc1d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "label.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhT04SSYkB5k"
      },
      "source": [
        "xi = tf.placeholder(tf.float32, [None, 9])\n",
        "xj = tf.placeholder(tf.float32, [None, 9])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKJ2phcHkB5n"
      },
      "source": [
        "Define the placeholder for label y,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3pqOWCEkB5n"
      },
      "source": [
        "y = tf.placeholder(tf.float32, [None, 1]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7iFlW6LkB5v"
      },
      "source": [
        "def embedding_function(x):\n",
        "    \n",
        "    weights = tf.Variable(tf.truncated_normal([9,1]))\n",
        "    bias = tf.Variable(tf.truncated_normal([1]))\n",
        "    \n",
        "    a = (tf.nn.xw_plus_b(x,weights,bias))\n",
        "    embeddings = tf.nn.relu(a)\n",
        "    \n",
        "    return embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3JV__xnkB5z"
      },
      "source": [
        "Compute the embeddings of our support set i.e $f_{\\varphi}(x_i) $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvcKOpQ6kB50"
      },
      "source": [
        "f_xi = embedding_function(xi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UwPyq-kkB54"
      },
      "source": [
        "Compute the embeddings of our query set i.e $f_{\\varphi}(x_j)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsvGr6v6kB55"
      },
      "source": [
        "f_xj = embedding_function(xj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w4nQm-DkB59"
      },
      "source": [
        "Now that we calculated the embeddings and have the feature vectors, we combine both the support set and query set feature vectors i.e $Z(f_{\\varphi}(x_i), f_{\\varphi}(x_j))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U76SlfrRkB5-"
      },
      "source": [
        "Z = tf.concat([f_xi,f_xj],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb7j3nFskB6B"
      },
      "source": [
        "We define our relation function $g_{\\phi}()$ as three layered neural network with relu activations, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyHeN4Z-kB6C"
      },
      "source": [
        "def relation_function(x):\n",
        "    w1 = tf.Variable(tf.truncated_normal([2,3]))\n",
        "    b1 = tf.Variable(tf.truncated_normal([3]))\n",
        "    \n",
        "    w2 = tf.Variable(tf.truncated_normal([3,5]))\n",
        "    b2 = tf.Variable(tf.truncated_normal([5]))\n",
        "    \n",
        "    w3 = tf.Variable(tf.truncated_normal([5,1]))\n",
        "    b3 = tf.Variable(tf.truncated_normal([1]))\n",
        "    \n",
        "    #layer1\n",
        "    z1 = (tf.nn.xw_plus_b(x,w1,b1))\n",
        "    a1 = tf.nn.relu(z1)\n",
        "    \n",
        "    #layer2\n",
        "    z2 = tf.nn.xw_plus_b(a1,w2,b2)\n",
        "    a2 = tf.nn.relu(z2)\n",
        "    \n",
        "    #layer3\n",
        "    z3 = tf.nn.xw_plus_b(z2,w3,b3)\n",
        "\n",
        "    #output\n",
        "    y = tf.nn.sigmoid(z3)\n",
        "    \n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf8DHpjwkB6I"
      },
      "source": [
        "We now pass the concatenated feature vectors of support set and query set to the relation function and get the relation scores, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tug_pF4EkB6J"
      },
      "source": [
        "relation_scores = relation_function(Z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD3jsuX7kB6R"
      },
      "source": [
        "We define our loss function as mean squared error i.e squared difference between relation scores and actual y value. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUXI3FiakB6R"
      },
      "source": [
        "loss_function = tf.reduce_mean(tf.squared_difference(relation_scores,y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb3q5W8NkB6Z"
      },
      "source": [
        "We can minimize the loss using adam optimizer,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PqA0fQxkB6a"
      },
      "source": [
        "optimizer = tf.train.AdamOptimizer(0.1)\n",
        "train = optimizer.minimize(loss_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRbL7dRMkB6c"
      },
      "source": [
        "Now, let's start our tensorflow session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh3z8SZHkB6c"
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qWfOB6YkB6g"
      },
      "source": [
        "Now, we randomly sample data points for our support set $x_i$ and query set $x_j$ and train the network, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2qiRDvkhkB6g"
      },
      "source": [
        "for episode in range(1000):\n",
        "    _, loss_value = sess.run([train, loss_function], \n",
        "                             feed_dict={xi:data[:,0:9]+np.random.randn(*np.shape(data[:,0:9]))*0.05,\n",
        "                                        xj:data[:,9:]+np.random.randn(*np.shape(data[:,9:]))*0.05,\n",
        "                                        y:label})\n",
        "    if episode % 100 == 0:\n",
        "        print(\"Episode {}: loss {:.3f} \".format(episode, loss_value))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}