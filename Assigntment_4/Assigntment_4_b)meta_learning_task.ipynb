{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.11"
    },
    "colab": {
      "name": "Assigntment_4_b)meta_learning_task.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zjzsu2000/CMPE297_Sec49AdvanceDL/blob/master/Assigntment_4/Assigntment_4_b)meta_learning_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1M4ZweSWkjf"
      },
      "source": [
        "# Assigntment_4_b)meta_learning_task\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKekHK1JWkjh"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaD7Sn7uWkjl"
      },
      "source": [
        "### Understanding the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJs7ZVTSWkjm",
        "outputId": "87514e47-a067-4fa8-b872-22bb3b42decc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "Image.open('data/images/Japanese_(katakana)/character13/0608_01.png')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IOError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5f137fcbaaa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/images/Japanese_(katakana)/character13/0608_01.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/images/Japanese_(katakana)/character13/0608_01.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt39vaqVWkjq"
      },
      "source": [
        "the same alphabet in different variation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UgbcqyzLWkjq"
      },
      "source": [
        "Image.open('data/images/Japanese_(katakana)/character13/0608_13.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnJV3IPfWkjt"
      },
      "source": [
        "Let us see sanskrit alphabet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoygkGOQWkju"
      },
      "source": [
        "Image.open('data/images/Sanskrit/character13/0863_09.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAQIzXe-Wkjw"
      },
      "source": [
        "Image.open('data/images/Sanskrit/character13/0863_13.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPEhdKftWkjy"
      },
      "source": [
        "How can we convert this image into an array? we can use np.array function to convert these images into an array and then we reshape it to 28*28"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms1TUiyRWkjz"
      },
      "source": [
        "image_name = 'data/images/Sanskrit/character13/0863_13.png'\n",
        "alphabet, character, rotation = 'Sanskrit/character13/rot000'.split('/')\n",
        "rotation = float(rotation[3:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "32iD5URRWkj2"
      },
      "source": [
        "np.array(Image.open(image_name).rotate(rotation).resize((28, 28)), np.float32,copy=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKYKfZ7NWkj5"
      },
      "source": [
        "Now, that we have understood, what is in our dataset, let us load our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-VFf7KVWkj6"
      },
      "source": [
        "root_dir = 'data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-p3LPTsWkj8"
      },
      "source": [
        "We have the splitting details in the /data/omniglot/splits/train.txt file which has the language name, character number, rotation information and images in /data/omniglot/data/ directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdCj90wnWkj9"
      },
      "source": [
        "train_split_path = os.path.join(root_dir, 'splits', 'train.txt')\n",
        "\n",
        "with open(train_split_path, 'r') as train_split:\n",
        "    train_classes = [line.rstrip() for line in train_split.readlines()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5axdM5prWkkA"
      },
      "source": [
        "#number of classes\n",
        "no_of_classes = len(train_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWbRY654WkkC"
      },
      "source": [
        "Now we set the number of examples to 20, as we have 20 example per class in our dataset, and also we set image width and height to 28 x 28:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6J46QwfWkkC"
      },
      "source": [
        "#number of examples\n",
        "num_examples = 20\n",
        "\n",
        "#image width\n",
        "img_width = 28\n",
        "\n",
        "#image height\n",
        "img_height = 28\n",
        "channels = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMAU_QR0WkkF"
      },
      "source": [
        "Next, we initialize our training dataset with a shape as a number of classes, number of examples, image height and image width:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2PdoOOqWkkG"
      },
      "source": [
        "train_dataset = np.zeros([no_of_classes, num_examples, img_height, img_width], dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mei-MjUWkkI"
      },
      "source": [
        "Now, we read all the images, convert it to numpy array and store it our train_dataset array with their label and values, that is,  train_dataset = [label, values]:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GXRkEZxWkkJ"
      },
      "source": [
        "for label, name in enumerate(train_classes):\n",
        "    alphabet, character, rotation = name.split('/')\n",
        "    rotation = float(rotation[3:])\n",
        "    img_dir = os.path.join(root_dir, 'data', alphabet, character)\n",
        "    img_files = sorted(glob.glob(os.path.join(img_dir, '*.png')))\n",
        "  \n",
        "    \n",
        "    for index, img_file in enumerate(img_files):\n",
        "        values = 1. - np.array(Image.open(img_file).rotate(rotation).resize((img_width, img_height)), np.float32, copy=False)\n",
        "        train_dataset[label, index] = values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztGglOK5WkkL"
      },
      "source": [
        "train_dataset.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb60HV5WWkkN"
      },
      "source": [
        "Now that we have loaded our training data, we need to create embeddings for them. We generate the embeddings using a convolution operation as our inputs are images. So, we define a convolutional block with 64 filters with batch normalization and ReLU as the activation function. Followed by we perform max pooling operation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7TDk49IWkkO"
      },
      "source": [
        "def convolution_block(inputs, out_channels, name='conv'):\n",
        "\n",
        "    conv = tf.layers.conv2d(inputs, out_channels, kernel_size=3, padding='SAME')\n",
        "    conv = tf.contrib.layers.batch_norm(conv, updates_collections=None, decay=0.99, scale=True, center=True)\n",
        "    conv = tf.nn.relu(conv)\n",
        "    conv = tf.contrib.layers.max_pool2d(conv, 2)\n",
        "    \n",
        "    return conv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEfFwqOQWkkQ"
      },
      "source": [
        "Now, we define our embedding function which gives us the embedding comprising of four convolutional blocks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4A8qU0CWkkR"
      },
      "source": [
        "def get_embeddings(support_set, h_dim, z_dim, reuse=False):\n",
        "\n",
        "        net = convolution_block(support_set, h_dim)\n",
        "        net = convolution_block(net, h_dim)\n",
        "        net = convolution_block(net, h_dim) \n",
        "        net = convolution_block(net, z_dim) \n",
        "        net = tf.contrib.layers.flatten(net)\n",
        "        \n",
        "        return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpWggYrAWkkT"
      },
      "source": [
        "Remember, we don't use our whole dataset for training, since we are one shot learning, we sample some data points from each class as a support set and train the network using the support set in an episodic fashion. \n",
        "\n",
        "\n",
        "Now we define some of the important variables, we consider a 60-way 5-shot learning scenario:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25OysrAGWkkT"
      },
      "source": [
        "#number of classes\n",
        "num_way = 60  \n",
        "\n",
        "#number of examples per class for support set\n",
        "num_shot = 5  \n",
        "\n",
        "#number of query points\n",
        "num_query = 5 \n",
        "\n",
        "#number of examples\n",
        "num_examples = 20\n",
        "\n",
        "h_dim = 64\n",
        "\n",
        "z_dim = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15ua9oOyWkkV"
      },
      "source": [
        "Next, we initialize placeholders for our support set and query set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufSvyRFmWkkW"
      },
      "source": [
        "support_set = tf.placeholder(tf.float32, [None, None, img_height, img_width, channels])\n",
        "query_set = tf.placeholder(tf.float32, [None, None, img_height, img_width, channels])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZl7o9qsWkkY"
      },
      "source": [
        "And we store the shape of our support set and query set in support_set_shape and query_set_shape respectively:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTEaBtg4WkkY"
      },
      "source": [
        "support_set_shape = tf.shape(support_set)\n",
        "query_set_shape = tf.shape(query_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCZadCJLWkka"
      },
      "source": [
        "Get the number of classes and number of data points in the support set and number of data points in the query set for initializing our support and query sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr81-Ma1Wkka"
      },
      "source": [
        "num_classes, num_support_points = support_set_shape[0], support_set_shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St43KgMgWkkd"
      },
      "source": [
        "num_query_points = query_set_shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1h_13CIWkkh"
      },
      "source": [
        "Next, we define the placeholder for our label:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AErVcz2RWkkh"
      },
      "source": [
        "y = tf.placeholder(tf.int64, [None, None])\n",
        "\n",
        "#convert the label to one hot\n",
        "y_one_hot = tf.one_hot(y, depth=num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp7XGMzzWkkk"
      },
      "source": [
        "Now, we generate the embeddings for our support set using our embedding function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_kSEjJIWkkk"
      },
      "source": [
        "support_set_embeddings = get_embeddings(tf.reshape(support_set, [num_classes * num_support_points, img_height, img_width, channels]), h_dim, z_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcQy2IAbWkkm"
      },
      "source": [
        "We compute the prototype of each class which is the mean vector of the support set embeddings of the class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8qlsR2eWkkm"
      },
      "source": [
        "embedding_dimension = tf.shape(support_set_embeddings)[-1]\n",
        "\n",
        "class_prototype = tf.reduce_mean(tf.reshape(support_set_embeddings, [num_classes, num_support_points, embedding_dimension]), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N34L7KSLWkko"
      },
      "source": [
        "Next, we use our same embedding function for getting embeddings of the query set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "musBx6VVWkkp"
      },
      "source": [
        "query_set_embeddings = get_embeddings(tf.reshape(query_set, [num_classes * num_query_points, img_height, img_width, channels]), h_dim, z_dim, reuse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIxvA0WAWkks"
      },
      "source": [
        "Now that, we have the class prototype and query set embeddings, we define a distance function which gives us the distance between the class prototypes and query set embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb_QkkdvWkks"
      },
      "source": [
        "def euclidean_distance(a, b):\n",
        "\n",
        "    N, D = tf.shape(a)[0], tf.shape(a)[1]\n",
        "    M = tf.shape(b)[0]\n",
        "    a = tf.tile(tf.expand_dims(a, axis=1), (1, M, 1))\n",
        "    b = tf.tile(tf.expand_dims(b, axis=0), (N, 1, 1))\n",
        "    return tf.reduce_mean(tf.square(a - b), axis=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SGM8YZaWkky"
      },
      "source": [
        "Calculate the distance between the class prototype and query set embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFbvnSS8Wkkz"
      },
      "source": [
        "distance = euclidean_distance(class_prototype,query_set_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4TO5n1UWkk1"
      },
      "source": [
        "Next, we get the probability for each class as a softmax to the distance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d8am6ljWkk1"
      },
      "source": [
        "predicted_probability = tf.reshape(tf.nn.log_softmax(-distance), [num_classes, num_query_points, -1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cGR7QrcWkk3"
      },
      "source": [
        "Compute the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu88Q1O_Wkk4"
      },
      "source": [
        "loss = -tf.reduce_mean(tf.reshape(tf.reduce_sum(tf.multiply(y_one_hot, predicted_probability), axis=-1), [-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWNdyozRWkk7"
      },
      "source": [
        "Calculate accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuN9722SWkk8"
      },
      "source": [
        "accuracy = tf.reduce_mean(tf.to_float(tf.equal(tf.argmax(predicted_probability, axis=-1), y)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDKjM8vgWkk_"
      },
      "source": [
        "We use Adam optimizer for minimizing the loss:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByGje4b5Wkk_"
      },
      "source": [
        "train = tf.train.AdamOptimizer().minimize(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1wQ-zaRWklB"
      },
      "source": [
        "Now, we start our tensorflow session and train the model,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQRMhpfoWklC"
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13w1OwSsWklF"
      },
      "source": [
        "num_epochs = 20\n",
        "num_episodes = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "bwNV0YAlWklI"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        \n",
        "        # select 60 classes\n",
        "        episodic_classes = np.random.permutation(no_of_classes)[:num_way]\n",
        "        \n",
        "        support = np.zeros([num_way, num_shot, img_height, img_width], dtype=np.float32)\n",
        "        \n",
        "        query = np.zeros([num_way, num_query, img_height, img_width], dtype=np.float32)\n",
        "        \n",
        "        \n",
        "        for index, class_ in enumerate(episodic_classes):\n",
        "            selected = np.random.permutation(num_examples)[:num_shot + num_query]\n",
        "            support[index] = train_dataset[class_, selected[:num_shot]]\n",
        "            \n",
        "            # 5 querypoints per classs\n",
        "            query[index] = train_dataset[class_, selected[num_shot:]]\n",
        "            \n",
        "        support = np.expand_dims(support, axis=-1)\n",
        "        query = np.expand_dims(query, axis=-1)\n",
        "        labels = np.tile(np.arange(num_way)[:, np.newaxis], (1, num_query)).astype(np.uint8)\n",
        "        _, loss_, accuracy_ = sess.run([train, loss, accuracy], feed_dict={support_set: support, query_set: query, y:labels})\n",
        "        \n",
        "        if (episode+1) % 10 == 0:\n",
        "            print('Epoch {} : Episode {} : Loss: {}, Accuracy: {}'.format(epoch+1, episode+1, loss_, accuracy_))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}